# ─────────────────────────────────────────────────────────────────────────────
# NeuralClaw — Configuration
# NOTE: LLM provider/model are configured under `llm:` below, not at the root.
# ─────────────────────────────────────────────────────────────────────────────

agent:
  name: "NeuralClaw"
  version: "1.0.0"
  max_iterations_per_turn: 10
  max_turn_timeout_seconds: 300
  default_trust_level: "low"

llm:
  default_provider: "bytez"
  default_model: "openai/gpt-5"
  temperature: 0.4
  max_tokens: 4096

  # ── Retry (exponential backoff on transient errors) ──────────────────────
  # Retries on: LLMConnectionError (network blip), LLMRateLimitError (429).
  # Does NOT retry: LLMContextError, LLMInvalidRequestError (permanent).
  retry:
    max_attempts: 3    # total attempts (1 initial + 2 retries)
    base_delay: 1.0    # seconds before first retry
    max_delay: 30.0    # cap on backoff delay

  # ── Failover ─────────────────────────────────────────────────────────────
  # If the primary provider exhausts all retries, these are tried in order.
  # Each fallback also gets max_attempts retries.
  # Only providers with a valid API key / reachable endpoint are actually used.
  # Uncomment and add a provider to enable:
  # fallback_providers:
  #   - ollama     # fall back to local Ollama (no API key needed)
  fallback_providers: []

  providers:
    bytez:
      api_key_env: "BYTEZ_API_KEY"
    ollama:
      base_url: "http://localhost:11434"
    openai:
      base_url:
    anthropic: {}

memory:
  chroma_persist_dir: "./data/chroma"
  sqlite_path: "./data/sqlite/episodes.db"
  embedding_model: "BAAI/bge-small-en-v1.5"
  max_short_term_turns: 20
  relevance_threshold: 0.55

  # ── Compaction ────────────────────────────────────────────────────────────
  # When the session reaches this many turns, the CLI suggests /compact.
  # /compact asks the LLM for a ~5-sentence summary, then replaces all but
  # the most recent `compact_keep_recent` turns with that summary.
  # Set compact_after_turns to 0 to disable the suggestion entirely.
  compact_after_turns: 15
  compact_keep_recent: 4

tools:
  browser:
    headless: true
    user_agent: "Mozilla/5.0 (compatible; NeuralClaw/1.0)"
    timeout_ms: 15000
  terminal:
    working_dir: "./data/agent_files"
    default_timeout_seconds: 30
    docker_sandbox: false
    docker_image: "python:3.11-slim"
    # Add specific command names here if you need extra binaries beyond the
    # built-in allowlist. Using ["*"] disables the safety whitelist entirely
    # and is NEVER permitted. Leave empty unless you have a specific need.
    whitelist_extra: []
  filesystem:
    allowed_paths:
      - "./data/agent_files"

safety:
  default_permission_level: "read"
  require_confirmation_for:
    - "HIGH"
    - "CRITICAL"

mcp:
  servers:
    blender:
      transport: stdio
      command: "uvx"
      args: ["blender-mcp"]
      enabled: false

telegram:
  authorized_user_ids: []

scheduler:
  timezone: "UTC"
  max_concurrent_tasks: 3

logging:
  level: "INFO"
  log_dir: "./data/logs"
  max_file_size_mb: 100
  backup_count: 5
  console_output: false   # false = logs to file only, terminal stays clean
  # json_format auto-detected from tty; override here if needed
  # json_format: true
# ── Skills Retry Policy (Phase 6) ────────────────────────────────────────────
skills:
  retry:
    retryable_errors:
      - SkillTimeoutError
      - LLMRateLimitError
    max_attempts: 3
    base_delay: 1.0
    max_delay: 30.0
    jitter: true
    overrides:
      terminal_exec:
        max_attempts: 1   # never retry shell commands (not idempotent)
      web_fetch:
        max_attempts: 3