# ─────────────────────────────────────────────────────────────────────────────
# NeuralClaw — Configuration
# NOTE: LLM provider/model are configured under `llm:` below, not at the root.
# ─────────────────────────────────────────────────────────────────────────────

agent:
  name: "Gabii"
  version: "1.0.0"
  max_iterations_per_turn: 10
  max_turn_timeout_seconds: 300
  default_trust_level: "low"

llm:
  default_provider: "ollama"
  default_model: "qwen3:8b"
  temperature: 0.4
  max_tokens: 4096

  # ── Retry (exponential backoff on transient errors) ──────────────────────
  # Retries on: LLMConnectionError (network blip), LLMRateLimitError (429).
  # Does NOT retry: LLMContextError, LLMInvalidRequestError (permanent).
  retry:
    max_attempts: 3    # total attempts (1 initial + 2 retries)
    base_delay: 1.0    # seconds before first retry
    max_delay: 30.0    # cap on backoff delay

  # ── Failover ─────────────────────────────────────────────────────────────
  # If the primary provider exhausts all retries, these are tried in order.
  # Each fallback also gets max_attempts retries.
  # Only providers with a valid API key / reachable endpoint are actually used.
  # Uncomment and add a provider to enable:
  # fallback_providers:
  #   - ollama     # fall back to local Ollama (no API key needed)
  fallback_providers: []

  providers:
    bytez:
      api_key_env: "BYTEZ_API_KEY"
    ollama:
      base_url: "http://localhost:11434"
    openai:
      base_url:
    anthropic: {}

memory:
  chroma_persist_dir: "./data/chroma"
  sqlite_path: "./data/sqlite/episodes.db"
  embedding_model: "BAAI/bge-small-en-v1.5"
  max_short_term_turns: 20
  relevance_threshold: 0.55

  # ── Compaction ────────────────────────────────────────────────────────────
  # When the session reaches this many turns, the CLI suggests /compact.
  # /compact asks the LLM for a ~5-sentence summary, then replaces all but
  # the most recent `compact_keep_recent` turns with that summary.
  # Set compact_after_turns to 0 to disable the suggestion entirely.
  compact_after_turns: 15
  compact_keep_recent: 4

tools:
  browser:
    headless: true
    user_agent: "Mozilla/5.0 (compatible; NeuralClaw/1.0)"
    timeout_ms: 15000
  terminal:
    working_dir: "./data/agent_files"
    default_timeout_seconds: 30
    docker_sandbox: false
    docker_image: "python:3.11-slim"
    # Add specific command names here if you need extra binaries beyond the
    # built-in allowlist. Using ["*"] disables the safety whitelist entirely
    # and is NEVER permitted. Leave empty unless you have a specific need.
    whitelist_extra: []
  filesystem:
    allowed_paths:
      - "./data/agent_files"

safety:
  default_permission_level: "read"
  require_confirmation_for:
    - "HIGH"
    - "CRITICAL"

mcp:
  servers:
    blender:
      transport: stdio
      command: "uvx"
      args: ["blender-mcp"]
      enabled: false

telegram:
  authorized_user_ids: []

scheduler:
  timezone: "UTC"
  max_concurrent_tasks: 3
  # Heartbeat: proactive check-in that reads ~/neuralclaw/HEARTBEAT.md
  # Set heartbeat_enabled: false to disable entirely.
  heartbeat_enabled: true
  heartbeat_interval_minutes: 30

voice:
  enabled: true
  # Faster-Whisper model size. Options: tiny, tiny.en, base, base.en, small, small.en, medium
  # base.en is the recommended default: fast on CPU, accurate for English.
  whisper_model: "base.en"
  whisper_device: "cpu"         # cpu or cuda (cuda requires NVIDIA GPU + cuDNN)
  # Path to a Piper .onnx voice model file.
  # Download from: https://github.com/rhasspy/piper/releases
  # Example: ~/.local/share/piper/en_US-lessac-medium.onnx
  piper_model_path: ""
  sample_rate: 16000            # must be 8000, 16000, 32000, or 48000
  channels: 1
  vad_aggressiveness: 0         # 0-3 (0=least aggressive, 3=most aggressive)
  silence_duration_ms: 800      # ms of silence to mark end of utterance
  max_utterance_s: 30           # hard cap on utterance length
  min_utterance_ms: 300         # clips shorter than this are ignored
  # Phase G — hotword / wake word
  wake_word_enabled: false      # set true to enable always-on hotword detection
  wake_word_model: "hey_mycroft" # openwakeword model name or path to .onnx
  wake_sensitivity: 0.5         # 0.0–1.0; lower = more sensitive (more false positives)
  mic_device_index: 0        # null = system default mic

logging:
  level: "INFO"
  log_dir: "./data/logs"
  max_file_size_mb: 100
  backup_count: 5
  console_output: false   # false = logs to file only, terminal stays clean
  # json_format auto-detected from tty; override here if needed
  # json_format: true
# ── Skills Retry Policy (Phase 6) ────────────────────────────────────────────
skills:
  retry:
    retryable_errors:
      - SkillTimeoutError
      - LLMRateLimitError
    max_attempts: 3
    base_delay: 1.0
    max_delay: 30.0
    jitter: true
    overrides:
      terminal_exec:
        max_attempts: 1   # never retry shell commands (not idempotent)
      web_fetch:
        max_attempts: 3

# ── ClawHub Bridge Adapter ───────────────────────────────────────────────────
clawhub:
  enabled: true
  skills_dir: "./data/clawhub/skills"
  registry_url: "https://clawhub.ai"
  execution:
    allow_binary_skills: true
    auto_install_deps: false          # require explicit opt-in for auto-installing
    sandbox_binary_skills: true
  env:
    block_on_missing_env: true
    show_env_requirements_on_install: true
  risk_defaults:
    prompt_only: "LOW"
    api_http: "LOW"
    binary_execution: "HIGH"
    install_directive: "HIGH"