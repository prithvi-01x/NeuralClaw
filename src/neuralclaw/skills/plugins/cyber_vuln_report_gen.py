"""
skills/plugins/cyber_vuln_report_gen.py â€” Cyber: Vulnerability Report Generator

Generates a structured vulnerability assessment report in Markdown from
provided findings data. Pure document generation â€” no scanning performed here.

Risk: LOW â€” fs:write
"""
from __future__ import annotations
import asyncio, time
from datetime import datetime, timezone
from pathlib import Path
from typing import ClassVar
from neuralclaw.skills.base import SkillBase
from neuralclaw.skills.types import RiskLevel, SkillManifest, SkillResult, SkillValidationError

_SEVERITY_ORDER = {"critical":0,"high":1,"medium":2,"low":3,"info":4}

class CyberVulnReportGenSkill(SkillBase):
    manifest: ClassVar[SkillManifest] = SkillManifest(
        name="cyber_vuln_report_gen",
        version="1.0.0",
        description="Generate a structured vulnerability assessment report in Markdown from provided findings. Accepts a list of findings with severity, title, description, and remediation. Writes to ~/neuralclaw/reports/.",
        category="cybersecurity",
        risk_level=RiskLevel.LOW,
        capabilities=frozenset({"fs:write"}),
        timeout_seconds=15,
        parameters={"type":"object","properties":{
            "target":{"type":"string","description":"Assessment target (domain, IP, application name)."},
            "findings":{"type":"array","description":"List of findings.","items":{"type":"object","properties":{
                "title":{"type":"string"},"severity":{"type":"string","enum":["critical","high","medium","low","info"]},
                "description":{"type":"string"},"remediation":{"type":"string","default":""},"evidence":{"type":"string","default":""},
                "cvss_score":{"type":"number","default":0},
            },"required":["title","severity","description"]}},
            "assessor":{"type":"string","description":"Assessor name or tool.","default":"NeuralClaw"},
            "output_path":{"type":"string","description":"Output file path (default: ~/neuralclaw/reports/vuln_report_<target>_<ts>.md).","default":""},
        },"required":["target","findings"]},
    )

    async def validate(self, target: str, findings: list, **_) -> None:
        if not target.strip(): raise SkillValidationError("target must be non-empty.")
        if not findings: raise SkillValidationError("findings must contain at least one item.")

    async def execute(self, target: str, findings: list, assessor: str="NeuralClaw", output_path: str="", **kwargs) -> SkillResult:
        call_id = kwargs.get("_skill_call_id","")
        t_start = time.monotonic()
        now = datetime.now(tz=timezone.utc)
        ts = now.strftime("%Y%m%d_%H%M%S")
        ts_human = now.strftime("%Y-%m-%d %H:%M UTC")

        findings_sorted = sorted(findings, key=lambda f: _SEVERITY_ORDER.get(f.get("severity","info").lower(), 4))
        counts = {s:0 for s in ["critical","high","medium","low","info"]}
        for f in findings_sorted: counts[f.get("severity","info").lower()] += 1

        def _render() -> str:
            lines = [
                f"# Vulnerability Assessment Report",
                f"**Target:** {target}  ",
                f"**Assessor:** {assessor}  ",
                f"**Date:** {ts_human}  ",
                f"**Total Findings:** {len(findings_sorted)}  ",
                "",
                "## Executive Summary",
                "",
                "| Severity | Count |",
                "|----------|-------|",
            ]
            for sev in ["critical","high","medium","low","info"]:
                lines.append(f"| {sev.capitalize()} | {counts[sev]} |")
            lines += ["","## Findings",""]
            for i, f in enumerate(findings_sorted, 1):
                sev = f.get("severity","info").upper()
                icon = {"CRITICAL":"ðŸ”´","HIGH":"ðŸŸ ","MEDIUM":"ðŸŸ¡","LOW":"ðŸ”µ","INFO":"âšª"}.get(sev,"âšª")
                lines += [
                    f"### {i}. {icon} [{sev}] {f.get('title','')}",
                    "",
                    f"**Severity:** {sev}",
                ]
                if f.get("cvss_score"): lines.append(f"**CVSS Score:** {f['cvss_score']}")
                lines += ["","**Description:**","",f.get("description",""),""]
                if f.get("evidence"): lines += ["**Evidence:**","",f"```\n{f['evidence']}\n```",""]
                if f.get("remediation"): lines += ["**Remediation:**","",f.get("remediation",""),""]
                lines.append("---")
            lines += ["","## Disclaimer","",
                "*This report was generated by NeuralClaw. Findings should be verified before remediation.*",
                f"*Generated: {ts_human}*"]
            return "\n".join(lines)

        try:
            import re
            safe_target = re.sub(r"[^\w\-]","_",target)[:40]
            out = Path(output_path).expanduser() if output_path else Path(f"~/neuralclaw/reports/vuln_report_{safe_target}_{ts}.md").expanduser()
            content = _render()
            loop = asyncio.get_event_loop()
            def _write():
                out.parent.mkdir(parents=True, exist_ok=True)
                out.write_text(content)
            await loop.run_in_executor(None, _write)
            duration_ms = (time.monotonic()-t_start)*1000
            return SkillResult.ok(self.manifest.name, call_id, {
                "output_path":str(out),"target":target,"finding_count":len(findings_sorted),
                "critical":counts["critical"],"high":counts["high"],"medium":counts["medium"],"low":counts["low"]
            }, duration_ms=duration_ms)
        except BaseException as e:
            return SkillResult.fail(self.manifest.name, call_id, f"{type(e).__name__}: {e}", type(e).__name__,
                                    duration_ms=(time.monotonic()-t_start)*1000)
